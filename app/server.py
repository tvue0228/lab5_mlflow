# app/server.py
import mlflow
from fastapi import FastAPI
from pydantic import BaseModel, Field
from typing import List

# ---- Hard-coded config (simple, explicit) ----
MLFLOW_TRACKING_URI = "http://127.0.0.1:5000"
MODEL_NAME          = "iris-classifier"
MODEL_VERSION       = "1"

mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
MODEL_URI = f"models:/{MODEL_NAME}/{MODEL_VERSION}"
model = mlflow.pyfunc.load_model(MODEL_URI)

# ----- Pydantic schemas with helpful docs + examples -----
class IrisSample(BaseModel):
    sepal_length: float = Field(..., ge=0, description="Sepal length in cm")
    sepal_width:  float = Field(..., ge=0, description="Sepal width in cm")
    petal_length: float = Field(..., ge=0, description="Petal length in cm")
    petal_width:  float = Field(..., ge=0, description="Petal width in cm")

class PredictRequest(BaseModel):
    samples: List[IrisSample]

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "samples": [
                        {"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2},
                        {"sepal_length": 6.7, "sepal_width": 3.1, "petal_length": 4.7, "petal_width": 1.5},
                        {"sepal_length": 6.3, "sepal_width": 3.3, "petal_length": 6.0, "petal_width": 2.5}
                    ]
                }
            ]
        }
    }

# For convenience, return both class ids and human labels
IRIS_LABELS = {0: "setosa", 1: "versicolor", 2: "virginica"}

class PredictResponse(BaseModel):
    class_id: List[int]    # 0,1,2
    class_label: List[str] # setosa/versicolor/virginica

    model_config = {
        "json_schema_extra": {
            "examples": [
                {"class_id": [0, 1, 2], "class_label": ["setosa", "versicolor", "virginica"]}
            ]
        }
    }

class VersionRequest(BaseModel):
    model_version: str

class ModelResponse(BaseModel):
    model_name: str  
    model_version: str
    model_uri: str

app = FastAPI(
    title="Iris Classifier API",
    description="Predict Iris species from sepal/petal measurements (cm).",
    version="1.0.0",
)

@app.get("/health", tags=["health"])
def health():
    return {"status": "ok", "model_uri": MODEL_URI}

@app.post(
    "/predict",
    response_model=PredictResponse,
    tags=["prediction"],
    summary="Predict Iris species",
    description="Send one or more Iris samples; returns class id (0,1,2) and label (setosa, versicolor, virginica)."
)
def predict(req: PredictRequest) -> PredictResponse:
    print(req.samples)
    ids=[]
    labels=[]
    for sample in req.samples:
        result = model.predict([[
            sample.sepal_length,
            sample.sepal_width,
            sample.petal_length,
            sample.petal_width
            ]])
        print(result)
        ids.append(result[0])
        labels.append(IRIS_LABELS[result[0]])
    # TODO Run predict
    return PredictResponse(
        class_id=[],
        class_label=[]
    )
    
# TODO Add endpoint to get the current model serving version
@app.get("/version", 
    response_model=ModelResponse,
    tags=["health"],
    summary="Current Model Version",
    description="Get the Current Model Version"
    )
def version():
    return ModelResponse(
        model_name=MODEL_NAME,
        model_version=MODEL_VERSION,
        model_uri=MODEL_URI
    )
    
# TODO Add endpoint to update the serving version
@app.post(
    "/setModelVersion",
    tags=["prediction"],
    summary="Set Model Version",
    description="Set the Model Version for use."
    )
def setModelVersion(req: VersionRequest):
    global MODEL_VERSION, MODEL_URI, model
    
    try :
        TRY_MODEL_VERSION = req.model_version
        TRY_MODEL_URI = f"models:/{MODEL_NAME}/{TRY_MODEL_VERSION}"
        model = mlflow.pyfunc.load_model(TRY_MODEL_URI)
        MODEL_VERSION = req.model_version
        MODEL_URI = f"models:/{MODEL_NAME}/{MODEL_VERSION}"
        
        return ModelResponse(
            model_name=MODEL_NAME,
            model_version=MODEL_VERSION,
            model_uri=MODEL_URI
        )

    except Exception as e :
        return {"status": "error", "status message": f"Unable to load model version - {e.message}"}
    
    
# TODO Predict using the correct served version
